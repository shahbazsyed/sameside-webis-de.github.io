---
layout: default
nav_active: index
title: Same Side Stance Classification - Webis Group
description: Same Side Stance Classification
---

<div class="uk-section uk-section-default">
    <div class="uk-container">
           
        <h1>Overview</h1>
<p>
Identifying (classifying) the stance of an argument towards a particular topic is a fundamental task in computational argumentation. The stance of an argument as considered here is a two-valued function: it can either be ''pro'' or ''con'' a topic. With the new task '<i>same side</i> (stance) classification' we address a simpler variant of this problem: Given two arguments regarding a certain topic, the task is to decide whether or not the two arguments have the same stance.
</p>
<p>
Current stance classification algorithms require knowledge about the topic the argument is about, i.e., the classifiers must be trained for a particular topic and hence cannot be reliably applied across topics. Observe that same side classification needs not to distinguish between topic-specific pro- and con-vocabulary respectively but ''only'' the argument similarity <i>within</i> a stance needs to be assessed. Probably, the problem can be solved indepently of a topic or a domain, so to speak, in a <i>topic-agnostic</i> manner. We believe that the development of such a technology has game-changing potential. It could be used to tackle, for instance, the following tasks&mdash;independent of a topic or a domain: measure bias strength within an argumentation, structure a discussion, find out who or what is challenging in a discussion, or filter wrongly labeled arguments in a large argument corpus.
</p>
<p>	
We invite you to participate in the 'same side stance classification' task! To keep the effort low, participants are only required to submit a short description of their models along with the predicted labels for the provided test set. The results of the task will be presented at the <a href="https://argmining19.webis.de/" target="_blank">6th Workshop on Argument Mining at ACL2019</a>, where also a discussion of this task and the received submissions is planned. Follow-up challenges if this task, which will, among others, include much larger datasets to allow for deep learning, are in preparation and will advertised after we have collected your feedback.
</p>
<p>
To participate, please click on the register button below. At the end of the registration, you will be able to download the training and test datasets along with code to quickly set-up your own experiments.
</p>
      <div id="register">

          <p style="float:left">
            <a href="https://forms.gle/gKgMqtrHJojJHrZf9" class="uk-button uk-button-primary"  target="_blank">Register
            </a>
            <a href="https://github.com/webis-de/argmining19-same-side-classification" target="_blank"
            class="uk-button uk-button-primary" >Git</a>
          </p>
          <br>
          <br>
</div>
<br>
<hr>
<h1>Dates</h1>
<ul class="uk-list">
    <li>June 5th, 2019: Training data available, competition begins. </li>
    <li>June 6th, 2019: Submission open.</li>
    <li>July 12th, 2019: Submission closed (23:59 PM UTC), manual evaluation begins.</li>
    <li>August 1st, 2019: Results (accuracy and model overview) presented at <a href="https://argmining19.webis.de/"
    target="_blank">the 6th Workshop on Argument Mining</a>.</li>
</ul>
<b>All deadlines are 11:59PM UTC-12:00 ("anywhere on earth").</b>
<hr>
<h1>Task Description</h1>
        <h2>Dataset and Experimental Settings</h2>
              <p>The dataset used in the task are derived from the following four sources: idebate.org,
                debatepedia.org, debatewise.org and debate.org. Each instance in the dataset holds the following fields: <br>
                <ul>
                  <li><i>id</i>: The id of the instance <br></li>
                  <li><i>topic</i>: The title of the debate. It can be a general topic (e.g. abortion) or a topic with a
                    stance (e.g. abortion should be legalized). <br></li>
                  <li> <i>argument1</i>: A pro or con argument related to the topic. <br></li>
                  <li> <i>argument1_id</i>: The ID of argument1. <br></li>
                  <li> <i>argument2</i>: A pro or con argument related to the topic. <br></li>
                  <li> <i>argument2_id</i>: The ID of argument2. <br></li>
                  <li> <i>is_same_stance</i>: True or False. True in case argument1 and argument2 have the same stance
                    towards the topic and False otherwise.<br></li>

                  
                </ul>
              </p>
    
              <p>
                We choose to work with the two most discussed topics in the considered four sources: <i>abortion</i> and <i>gay marriage</i>.
                Two experiments are set-up for the task of <i>same side stance classification</i>:
                <ul>
                  <li><i>Within Topics</i>: The training set contains arguments for a set of topics (<i>abortion</i> and
                    <i>gay marriage</i>) and the test set contains arguments related to the same set of topics
                    (<i>abortion</i> and <i>gay marriage</i>). The following table presents an overview about the data. <br>
                    <table class="uk-margin-medium uk-table uk-table-divider uk-table-small sameside-table-normal">
                        <tr>
                        <th><br>Class</th>
                        <th class="numeric">Topic: Abortion<br>&nbsp;</th>
                        <th class="numeric">Topic: Gay Marriage<br>&nbsp;</th>
                      </tr>
                      <tr>
                        <td> Same Side</td>
                        <td class="numeric">20,834</td>
                        <td class="numeric">13,277</td>
                      </tr>
                      <tr>
                        <td> Different Side</td>
                        <td class="numeric">20,006</td>
                        <td class="numeric">9,786</td>
                      </tr>
                      <tr>
                        <td> <b>Total</b></td>
                        <td class="numeric"><b>40,840</b></td>
                        <td class="numeric"><b>23,063</b></td>
                      </tr>
                    </table>
    
                  </li>
                  <li><i>Cross Topics</i>: The training set contains arguments for a topic (<i>abortion</i>) and the test
                    set contains argument related to the another set of topics
    
                    <table class="uk-margin-medium uk-table uk-table-divider uk-table-small sameside-table-small">
                      <tr>
                        <th>Class</th>
                        <th class="numeric"># of instances</th>
                      </tr>
                      <tr>
                        <td> Same Side</td>
                        <td class="numeric">31,195</td>
                      </tr>
                      <tr>
                        <td> Different Side</td>
                        <td class="numeric">29,853</td>
                      </tr>
                      <tr>
                        <td> <b>Total</b></td>
                        <td class="numeric"><b>61,048</b></td>
                      </tr>
                    </table>
    
    
    
                  </li>
                </ul>
    
    
              </p>
              <h2>Evaluation</h2>
            <p class="card-text text-left">We provide a test set for each experiment setting (<i>Within Topics</i> and <i>Across topics</i>). After you train your model,
              predict the labels of the test set and send us the results as a CSV file, comma seperated, with header
              <i>id</i> and <i>label</i>. The <i>id</i> is the instance id provided in the test set and the <i>label</i>
              has a value of <b>True</b> (the two arguments have the same side) or <b>False</b> (the two arguments do not
              have the same side). The results will be evaluated using <strong>accuracy</strong>.
            </p>
    </div>
</div>
