---
layout: default
nav_active: leaderboard
title: Same Side Stance Classification - Webis Group
description: Same Side Stance Classification
---

<main class="uk-section uk-section-default">

    <div class="uk-container">

            <h1>Leaderboard</h1>
            <p>We show the best performing model (based on accuracy) for each team, sorted by highest accuracy. We plan to publish the results of this shared task (details will follow).</p>
<h2> Results <i>Within Topics</i> </h2>

<p>We evaluate all submitted predictions using a balanced subset of of the original <i>test</i> set. </p>

<p>To see more details about each model, click on each row.</p>
<table id="within-leaderboard" class="leaderboard uk-table  uk-table-hover uk-table-condensed">

    <thead>

        <tr>
            <th>#</th>
            <th>Team</th>
            <th>University</th>
            <th>Precision</th>
            <th>Recall</th>
            <th>Accuracy</th>
        </tr>
    </thead>
    <tbody>
        <tr class="mainrow" id="1">
            <td>1</td>
            <td>ReCAP</td>
            <td>Trier University </td>
            <td>0.85</td>
            <td>0.66</td>
            <td> 0.77</td>
        </tr>
        <tr class="subrow" name="1">
            <td colspan="3"><i>Abortion</i></td>
            <td>0.79</td>
            <td>0.59</td>
            <td>0.71</td>
        </tr>
        <tr class="subrow" name="1">
            <td colspan="3"><i>Gay Marriage</i></td>
            <td>0.90</td>
            <td>0.73</td>
            <td> 0.83</td>
        </tr>
        <tr class="subrow no-align" name="1">
                <td colspan="6"><b>Model overview:</b> BERT (large, uncased, sequence length 512), tuning for 3 epochs.</td>
                </tr>
        <tr id="2">
            <td>2</td>
            <td>ASV</td>
            <td>Leipzig University </td>
            <td>0.79</td>
            <td>0.73</td>
            <td> 0.77</td>
        </tr>
        <tr class="subrow" name="2">
            <td colspan="3"><i>Abortion</i></td>
            <td>0.78</td>
            <td>0.68</td>
            <td>0.75</td>
        </tr>
        <tr class="subrow" name="2">
            <td colspan="3"><i>Gay Marriage</i></td>
            <td>0.80</td>
            <td>0.78</td>
            <td> 0.79</td>
        </tr>
        <tr class="subrow no-align" name="2">
                <td colspan="6"><b>Model overview:</b> BERT (uncased, sequence length 512, tuning for 5 epochs), loss function: sigmoid_binary_crossentrophy.</td>
                </tr>
        <tr id="3">
            <td>3</td>
            <td>IBM Research </td>
            <td>IBM Research </td>
            <td>0.69</td>
            <td>0.59</td>
            <td> 0.66</td>
        </tr>
        <tr class="subrow" name="3">
            <td colspan="3"><i>Abortion</i></td>
            <td>0.64</td>
            <td>0.54</td>
            <td>0.62</td>
        </tr>
        <tr class="subrow" name="3">
            <td colspan="3"><i>Gay Marriage</i></td>
            <td>0.73</td>
            <td>0.63</td>
            <td> 0.70</td>
        </tr>
        <tr class="subrow no-align" name="3">
                <td colspan="6"><b>Model overview:</b> Two BERT models fine-tuned in cascade starting from the vanilla BERT model.</td>
                </tr>
        <tr id="4">
            <td>4</td>
            <td>UKP</td>
            <td>TU Darmstadt </td>
            <td>0.68</td>
            <td>0.52</td>
            <td> 0.64</td>
        </tr>
        <tr class="subrow" name="4">
            <td colspan="3"><i>Abortion</i></td>
            <td>0.63</td>
            <td>0.48</td>
            <td>0.60</td>
        </tr>
        <tr class="subrow" name="4">
            <td colspan="3"><i>Gay Marriage</i></td>
            <td>0.74</td>
            <td>0.56</td>
            <td> 0.68</td>
        </tr>
        <tr class="subrow no-align" name="4">
                <td colspan="6"><b>Model overview:</b> Microsoft's Multi-Task Deep Neural Network mt-dnn. Basis for the mt-dnn is BERT (large). No hyper-parameter tuning, 4 epochs.</td>
                </tr>
        <tr id="5">
            <td>5</td>
            <td>HHU SSSC</td>
            <td>Düsseldorf University </td>
            <td>0.70</td>
            <td>0.33</td>
            <td> 0.60</td>
        </tr>
        <tr class="subrow" name="5">
            <td colspan="3"><i>Abortion</i></td>
            <td>0.65</td>
            <td>0.32</td>
            <td>0.57</td>
        </tr>
        <tr class="subrow" name="5">
            <td colspan="3"><i>Gay Marriage</i></td>
            <td>0.76</td>
            <td>0.35</td>
            <td> 0.62</td>
        </tr>
        <tr class="subrow no-align" name="5">
                <td colspan="6"><b>Model overview:</b> Manhattan LSTM -- a siamese network -- which measures the similarity of both arguments. Document embeddings via BERT (base, uncased, not fine-tuned, sequence length 512 tokens).</td>
                </tr>
        <tr id="6">
            <td>6</td>
            <td>DBS</td>
            <td>LMU*</td>
            <td>0.53</td>
            <td>1.00</td>
            <td> 0.55</td>
        </tr>
        <tr class="subrow" name="6">
            <td colspan="3"><i>Abortion</i></td>
            <td>0.53</td>
            <td>1.00</td>
            <td>0.55</td>
        </tr>
        <tr class="subrow" name="6">
            <td colspan="3"><i>Gay Marriage</i></td>
            <td>0.53</td>
            <td>1.00</td>
            <td> 0.55</td>
        </tr>
        <tr class="subrow no-align" name="6">
                <td colspan="6"><b>Model overview:</b> Bert (base). Arguments organized as graph: edges are weighted with the confidence that arguments agree and confidence that they disagree. If known from training set that the arguments agree or disagree the confidence is 0 and 1 or 1 and 0 accordingly.</td>
                </tr>
        <tr id="7">
            <td>7</td>
            <td>ACQuA</td>
            <td>MLU Halle </td>
            <td>0.53</td>
            <td>0.57</td>
            <td> 0.54</td>
        </tr>
        <tr class="subrow" name="7">
            <td colspan="3"><i>Abortion</i></td>
            <td>0.53</td>
            <td>0.57</td>
            <td>0.53</td>
        </tr>
        <tr class="subrow" name="7">
            <td colspan="3"><i>Gay Marriage</i></td>
            <td>0.54</td>
            <td>0.57</td>
            <td> 0.54</td>
        </tr>
        <tr class="subrow no-align" name="7">
                <td colspan="6"><b>Model overview:</b> Rule-based: same-side classification is considered as a sentiment analysis task. Arguments with negative as well as positive sentiments are on the same side. Vocabulary withf positive and negative words from Minqing Hu and Bing Liu.</td>
                </tr>
        <tr id="8">
            <td>8</td>
            <td>Paderborn University</td>
            <td>Paderborn University </td>
            <td>0.59</td>
            <td>0.19</td>
            <td> 0.53</td>
        </tr>
        <tr class="subrow" name="8">
            <td colspan="3"><i>Abortion</i></td>
            <td>0.62</td>
            <td>0.21</td>
            <td>0.54</td>
        </tr>
        <tr class="subrow" name="8">
            <td colspan="3"><i>Gay Marriage</i></td>
            <td>0.55</td>
            <td>0.17</td>
            <td> 0.52</td>
        </tr>
        <tr class="subrow no-align" name="8">
                <td colspan="6"><b>Model overview:</b> Siamese Neural Network as a benchmark model. Embedding via Flair library.</td>
                </tr>
        <tr id="9">
            <td>9</td>
            <td>sam</td>
            <td>Postdam University </td>
            <td>0.51</td>
            <td>0.58</td>
            <td> 0.51</td>
        </tr>
        <tr class="subrow" name="9">
            <td colspan="3"><i>Abortion</i></td>
            <td>0.56</td>
            <td>0.62</td>
            <td>0.56</td>
        </tr>
        <tr class="subrow" name="9">
            <td colspan="3"><i>Gay Marriage</i></td>
            <td>0.46</td>
            <td>0.54</td>
            <td> 0.45</td>
        </tr>
        <tr class="subrow no-align" name="9">
                <td colspan="6"><b>Model overview:</b> Bidirectional LSTM with 512 hidden units. Embedded sentences are decided via a two-layer MLP.</td>
                </tr>
    </tbody>
</table>

<p>* In addition to their Bert model the DBS team from LMU exploited also the fact that the "same side" relation is an equivalence relation: if a test set contains arguments from the training set, labels can be deduced via the transitivity property (among others). However, the test set underlying the results shown above does not contain such exploitable algebraic or logical structures in order to compare - as intended - the language processing power of the submissions. We would like to mention this fact since the DBS team informed us about their use of this possibility beforehand, which shows their fair-mindedness, and, not least, since their exploitation of the algebraic structure is a smart move.</p>

<hr>


<h2> Results <i>Cross Topics</i> </h2>
<p>We evaluate all submitted predictions on a balanced subset of the <i>Cross</i> domain <i>test</i> set:
 <!--   <ul>
            <li><b>Balanced Test Set:</b> balanced subset of the original <i>cross</i> domain <i>test</i> set.</li>
            <li><b>Balanced/No Leakage Test Set:</b> a subset which can not be leaked from the <i>within</i> domain <i>training</i> set. </li>
    </ul>-->
</p>
<table id="within-leaderboard" class="leaderboard uk-table  uk-table-hover uk-table-condensed">

    <thead>
   
        <tr>
            <th>#</th>
            <th>Team</th>
            <th>University</th>
            <th>Precision</th>
            <th>Recall</th>
            <th>Accuracy</th>
        </tr>
    </thead>
    <tbody>
        <tr class="mainrow">
            <td>1</td>
            <td>HHU SSSC</td>
            <td>Düsseldorf University </td>
            <td>0.72</td>
            <td>0.53</td>
            <td>0.66</td>
        <!--  <td>0.68</td>
            <td>0.37</td>
            <td>0.60</td>--> 
        </tr>

        <tr >
            <td>2</td>
            <td>DBS</td>
            <td>LMU </td>
            <td>0.67</td>
            <td>0.53</td>
            <td>0.63</td>
         <!--   <td>0.78</td>
            <td>0.61</td>
            <td>0.72</td>-->
        </tr>
        <tr>
            <td>2</td>
            <td>UKP</td>
            <td>TU Darmstadt </td>
            <td>0.64</td>
            <td>0.59</td>
            <td>0.63</td>
          <!--  <td>0.71</td>
            <td>0.63</td>
            <td>0.68</td> -->
        </tr>
        <tr>
            <td>3</td>
            <td>IBM Research</td>
            <td>IBM Research </td>
            <td>0.62</td>
            <td>0.49</td>
            <td>0.60</td>
         <!--   <td>0.74</td>
            <td>0.43</td>
            <td>0.64</td>-->
        </tr>
        <tr>
            <td>4</td>
            <td>Paderborn University </td>
            <td>Paderborn University </td>
            <td>0.60</td>
            <td>0.38</td>
            <td>0.56</td>
        <!--    <td>0.79</td>
            <td>0.33</td>
            <td>0.62</td>-->
        </tr>
        <tr>
            <td>5</td>
            <td>ReCAP</td>
            <td>Trier University </td>
            <td>0.69</td>
            <td>0.16</td>
            <td>0.54</td>
         <!--   <td>1.00</td>
            <td>0.20</td>
            <td>0.60</td>-->
        </tr>
        
        <tr>
            <td>6</td>
            <td>ACQuA</td>
            <td>MLU Halle </td>
            <td>0.50</td>
            <td>0.57</td>
            <td>0.50</td>
        <!--    <td>0.60</td>
            <td>0.54</td>
            <td>0.59</td>--> 
        </tr>
        <tr>
            <td>7</td>
            <td>sam</td>
            <td>Postdam University </td>
            <td>0.51</td>
            <td>0.52</td>
            <td>0.51</td>
        <!--    <td>0.57</td>
            <td>0.65</td>
            <td>0.58</td> -->
        </tr>
       <!--  <tr class="dismissed">
            <td>NA</td>
            <td>ASV*</td>
            <td>Leipzig University </td>
            <td>0.93</td>
            <td>0.90</td>
            <td>0.92</td>
          -->
        </tr>
    </tbody>
</table>
<!-- <p>* Dismissed because trained on <i>within</i> training set</p> -->
<hr>

</div>
</main>